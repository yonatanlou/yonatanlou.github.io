<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<title>Yonatan Lourie</title>
		<meta name="description" content="In case you got curious and want to do some cool stuff with one of the ancient texts ever found.">
		<link rel="alternate" href="feed/feed.xml" type="application/atom+xml" title="Yonatan Lourie">
		
		<style>/**
 * okaidia theme for JavaScript, CSS and HTML
 * Loosely based on Monokai textmate theme by http://www.monokai.nl/
 * @author ocodia
 */

code[class*="language-"],
pre[class*="language-"] {
	color: #f8f8f2;
	background: none;
	text-shadow: 0 1px rgba(0, 0, 0, 0.3);
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	font-size: 1em;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
	border-radius: 0.3em;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #272822;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: #8292a2;
}

.token.punctuation {
	color: #f8f8f2;
}

.token.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.constant,
.token.symbol,
.token.deleted {
	color: #f92672;
}

.token.boolean,
.token.number {
	color: #ae81ff;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
	color: #a6e22e;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string,
.token.variable {
	color: #f8f8f2;
}

.token.atrule,
.token.attr-value,
.token.function,
.token.class-name {
	color: #e6db74;
}

.token.keyword {
	color: #66d9ef;
}

.token.regex,
.token.important {
	color: #fd971f;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}
/*
 * New diff- syntax
 */

pre[class*="language-diff-"] {
	--eleventy-code-padding: 1.25em;
	padding-left: var(--eleventy-code-padding);
	padding-right: var(--eleventy-code-padding);
}
.token.deleted {
	background-color: hsl(0, 51%, 37%);
	color: inherit;
}
.token.inserted {
	background-color: hsl(126, 31%, 39%);
	color: inherit;
}

/* Make the + and - characters unselectable for copy/paste */
.token.prefix.unchanged,
.token.prefix.inserted,
.token.prefix.deleted {
	-webkit-user-select: none;
	user-select: none;
	display: inline-flex;
	align-items: center;
	justify-content: center;
	padding-top: 2px;
	padding-bottom: 2px;
}
.token.prefix.inserted,
.token.prefix.deleted {
	width: var(--eleventy-code-padding);
	background-color: rgba(0,0,0,.2);
}

/* Optional: full-width background color */
.token.inserted:not(.prefix),
.token.deleted:not(.prefix) {
	display: block;
	margin-left: calc(-1 * var(--eleventy-code-padding));
	margin-right: calc(-1 * var(--eleventy-code-padding));
	text-decoration: none; /* override del, ins, mark defaults */
	color: inherit; /* override del, ins, mark defaults */
}
/* This is an arbitrary CSS string added to the bundle */
/* Defaults */
:root {
	--font-family: -apple-system, system-ui, sans-serif;
	--font-family-monospace: Consolas, Menlo, Monaco, Andale Mono WT, Andale Mono, Lucida Console, Lucida Sans Typewriter, DejaVu Sans Mono, Bitstream Vera Sans Mono, Liberation Mono, Nimbus Mono L, Courier New, Courier, monospace;
}

/* Theme colors */
:root {
	--color-gray-20: #e0e0e0;
	--color-gray-50: #C0C0C0;
	--color-gray-90: #333;

	--background-color: #fff;

	--text-color: var(--color-gray-90);
	--text-color-link: #082840;
	--text-color-link-active: #5f2b48;
	--text-color-link-visited: #17050F;

	--syntax-tab-size: 2;
}

@media (prefers-color-scheme: dark) {
	:root {
		--color-gray-20: #e0e0e0;
		--color-gray-50: #C0C0C0;
		--color-gray-90: #dad8d8;

		/* --text-color is assigned to --color-gray-_ above */
		--text-color-link: #1493fb;
		--text-color-link-active: #6969f7;
		--text-color-link-visited: #a6a6f8;

		--background-color: #15202b;
	}
}


/* Global stylesheet */
* {
	box-sizing: border-box;
}

@view-transition {
	navigation: auto;
}

html,
body {
	padding: 0;
	margin: 0 auto;
	font-family: var(--font-family);
	color: var(--text-color);
	background-color: var(--background-color);
}
html {
	overflow-y: scroll;
}
body {
	max-width: 40em;
}

/* https://www.a11yproject.com/posts/how-to-hide-content/ */
.visually-hidden {
	clip: rect(0 0 0 0);
	clip-path: inset(50%);
	height: 1px;
	overflow: hidden;
	position: absolute;
	white-space: nowrap;
	width: 1px;
}

p:last-child {
	margin-bottom: 0;
}
p {
	line-height: 1.5;
}

li {
	line-height: 1.5;
}

a[href] {
	color: var(--text-color-link);
}
a[href]:visited {
	color: var(--text-color-link-visited);
}
a[href]:hover,
a[href]:active {
	color: var(--text-color-link-active);
}

main,
footer {
	padding: 1rem;
}
main :first-child {
	margin-top: 0;
}

header {
	border-bottom: 1px dashed var(--color-gray-20);
}
header:after {
	content: "";
	display: table;
	clear: both;
}

.links-nextprev {
	display: flex;
	justify-content: space-between;
	gap: .5em 1em;
	list-style: "";
	border-top: 1px dashed var(--color-gray-20);
	padding: 1em 0;
}
.links-nextprev > * {
	flex-grow: 1;
}
.links-nextprev-next {
	text-align: right;
}

table {
	margin: 1em 0;
}
table td,
table th {
	padding-right: 1em;
}

pre,
code {
	font-family: var(--font-family-monospace);
}
pre:not([class*="language-"]) {
	margin: .5em 0;
	line-height: 1.375; /* 22px /16 */
	-moz-tab-size: var(--syntax-tab-size);
	-o-tab-size: var(--syntax-tab-size);
	tab-size: var(--syntax-tab-size);
	-webkit-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
	direction: ltr;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	overflow-x: auto;
}
code {
	word-break: break-all;
}

/* Header */
header {
	display: flex;
	gap: 1em .5em;
	flex-wrap: wrap;
	align-items: center;
	padding: 1em;
}
.home-link {
	font-size: 1em; /* 16px /16 */
	font-weight: 700;
	margin-right: 2em;
}
.home-link:link:not(:hover) {
	text-decoration: none;
}

/* Nav */
.nav {
	display: flex;
	padding: 0;
	margin: 0;
	list-style: none;
}
.nav-item {
	display: inline-block;
	margin-right: 1em;
}
.nav-item a[href]:not(:hover) {
	text-decoration: none;
}
.nav a[href][aria-current="page"] {
	text-decoration: underline;
}

/* Posts list */
.postlist {
	list-style: none;
	padding: 0;
	padding-left: 1.5rem;
}
.postlist-item {
	display: flex;
	flex-wrap: wrap;
	align-items: baseline;
	counter-increment: start-from -1;
	margin-bottom: 1em;
}
.postlist-item:before {
	display: inline-block;
	pointer-events: none;
	content: "" counter(start-from, decimal-leading-zero) ". ";
	line-height: 100%;
	text-align: right;
	margin-left: -1.5rem;
}
.postlist-date,
.postlist-item:before {
	font-size: 0.8125em; /* 13px /16 */
	color: var(--color-gray-90);
}
.postlist-date {
	word-spacing: -0.5px;
}
.postlist-link {
	font-size: 1.1875em; /* 19px /16 */
	font-weight: 700;
	flex-basis: calc(100% - 1.5rem);
	padding-left: .25em;
	padding-right: .5em;
	text-underline-position: from-font;
	text-underline-offset: 0;
	text-decoration-thickness: 1px;
}
.postlist-item-active .postlist-link {
	font-weight: bold;
}

/* Tags */
.post-tag {
	display: inline-flex;
	align-items: center;
	justify-content: center;
	text-transform: capitalize;
	font-style: italic;
}
.postlist-item > .post-tag {
	align-self: center;
}

/* Tags list */
.post-metadata {
	display: inline-flex;
	flex-wrap: wrap;
	gap: .5em;
	list-style: none;
	padding: 0;
	margin: 0;
}
.post-metadata time {
	margin-right: 1em;
}</style>
	</head>
	<body>
		<a href="#skip" class="visually-hidden">Skip to main content</a>

		<header>
			<a href="/eleventy-base-blog/" class="home-link">Yonatan Lourie</a>
			<nav>
				<h2 class="visually-hidden" id="top-level-navigation-menu">Top level navigation menu</h2>
				<ul class="nav">
					<li class="nav-item"><a href="/eleventy-base-blog/">Home</a></li>
					<li class="nav-item"><a href="/eleventy-base-blog/blog/">Archive</a></li>
					<li class="nav-item"><a href="/eleventy-base-blog/about/">About</a></li>
					<li class="nav-item"><a href="/eleventy-base-blog/quality-content/">Quality content</a></li>
					<li class="nav-item"><a href="/eleventy-base-blog/feed/feed.xml">Feed</a></li>
				</ul>
			</nav>
		</header>

		<main id="skip">
			<heading-anchors>
				
<h1 id="how-to-use-the-qumran-scrolls-for-nlp-tasks-using-text-fabric">How to use the Qumran scrolls for NLP tasks using text-fabric</h1>

<ul class="post-metadata">
	<li><time datetime="2023-10-30">30 October 2023</time></li>
</ul>

<p>When I first started my thesis, I inherited the Dead Sea Scrolls project from another student in my lab. He provided a large file that looked something like this: <picture><source type="image/avif" srcset="/eleventy-base-blog/blog/data-pipeline-for-qumran-scrolls/data-pipeline-for-the-qumran-scrolls-using-text-fabric/l__zRaStbu-510.avif 510w"><source type="image/webp" srcset="/eleventy-base-blog/blog/data-pipeline-for-qumran-scrolls/data-pipeline-for-the-qumran-scrolls-using-text-fabric/l__zRaStbu-510.webp 510w"><img loading="lazy" decoding="async" src="/eleventy-base-blog/blog/data-pipeline-for-qumran-scrolls/data-pipeline-for-the-qumran-scrolls-using-text-fabric/l__zRaStbu-510.png" alt="abeg_origin" width="510" height="340"></picture></p>
<p>I also inherited a complex &quot;spaghetti&quot; code that parsed this data into readable Hebrew text. These files (from the image above),  were created by Martin Abegg and contain all of the text of the Dead Sea Scrolls, with morphological data on each word.</p>
<p>A few weeks later, I discovered the text-fabric package, a toolkit for analyzing large, structured text datasets, including ancient manuscripts, biblical texts, and other linguistically complex corpora. Popular among researchers in linguistics, biblical studies, and digital humanities, it efficiently manages intricate text structures and large datasets.</p>
<p>This package actually parsing those files that looks like gibberish from Abegg, but with much more organized and easy to use code.</p>
<p>For my purposes, I needed the entire corpus as plain text, along with the morphological features of each word (similar to Named Entity Recognition or NER). Here’s a quick guide on how to set it up:</p>
<p><strong>Disclaimer</strong>
The documentation for this package is solid, but i felt that the learning curve was to steep for what I needed. This post is simply if you need to do something similiar to mine.</p>
<h2 id="installation">Installation</h2>
<p>Follow the instructions in <a href="https://annotation.github.io/text-fabric/tf/about/install.html">here</a>.
If you having problems, you can try to install it with: <code>pip instal 'text-fabric[github]'</code>.</p>
<h3 id="installation-and-setup">Installation and Setup</h3>
<p>(For more comprehensive guide check the <a href="https://annotation.github.io/text-fabric/tf/index.html">documentation</a>)
To get started, you’ll need to install <code>text-fabric</code> and load a dataset.
Here’s an example using the ETCBC Hebrew Bible dataset:</p>
<pre class="language-python" tabindex="0"><code class="language-python">!pip install text<span class="token operator">-</span>fabric
<span class="token comment"># If it will not work, you can try:</span>
!pip instal <span class="token string">'text-fabric[github]'</span>

<span class="token comment"># Import Text-Fabric and load a dataset</span>
<span class="token keyword">from</span> tf<span class="token punctuation">.</span>app <span class="token keyword">import</span> use

<span class="token comment"># Load the Hebrew Bible (ETCBC dataset)</span>
A <span class="token operator">=</span> use<span class="token punctuation">(</span><span class="token string">'etcbc/bhsa'</span><span class="token punctuation">,</span> hoist<span class="token operator">=</span><span class="token builtin">globals</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>
<p><strong>Some basic operations:</strong></p>
<ul>
<li>Get the Text Structure: You can load text elements by type (book, chapter, verse) and navigate through them.</li>
</ul>
<pre class="language-python" tabindex="0"><code class="language-python"><span class="token comment"># Get all books</span>
<span class="token keyword">for</span> book <span class="token keyword">in</span> F<span class="token punctuation">.</span>otype<span class="token punctuation">.</span>s<span class="token punctuation">(</span><span class="token string">'book'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>F<span class="token punctuation">.</span>book<span class="token punctuation">.</span>v<span class="token punctuation">(</span>book<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># Prints book names</span></code></pre>
<ul>
<li>Accessing Specific Text Data: Let’s retrieve a verse in Genesis.</li>
</ul>
<pre class="language-python" tabindex="0"><code class="language-python"><span class="token comment"># Get Genesis 1:1</span>
verse_node <span class="token operator">=</span> T<span class="token punctuation">.</span>nodeFromSection<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">'Genesis'</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>T<span class="token punctuation">.</span>text<span class="token punctuation">(</span>verse_node<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># Outputs the text of Genesis 1:1</span>
</code></pre>
<ul>
<li>Accessing Metadata: You can retrieve metadata for specific elements like morphology or part of speech.</li>
</ul>
<pre class="language-python" tabindex="0"><code class="language-python"><span class="token comment"># Get morphological and part of speech info for a specific word in Genesis 1:1</span>
word_node <span class="token operator">=</span> L<span class="token punctuation">.</span>d<span class="token punctuation">(</span>verse_node<span class="token punctuation">,</span> otype<span class="token operator">=</span><span class="token string">'word'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># First word in Genesis 1:1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>F<span class="token punctuation">.</span>lex<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word_node<span class="token punctuation">)</span><span class="token punctuation">,</span> F<span class="token punctuation">.</span>sp<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word_node<span class="token punctuation">)</span><span class="token punctuation">,</span> F<span class="token punctuation">.</span>g_cons<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word_node<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<h2 id="implementation">Implementation</h2>
<p>For my case, i want to use the DSS dataset, the implementation is pretty straight forward, run over each scroll, store each word as a dict (each word have few attributes). That means we eventually end up with a dict of dict of dicts</p>
<pre class="language-cmd" tabindex="0"><code class="language-cmd">{Scroll1:[{text:ושמע, lex:שמע, ...}, {text:ה׳, lex:ה, ...}], 
Scroll2:[{text:רצה, lex:רצה, ...}, {text:משה, lex:משה, ...}]}</code></pre>
<p>Where each item in the list of each scroll is a word.</p>
<pre class="language-python" tabindex="0"><code class="language-python"><span class="token keyword">from</span> collections <span class="token keyword">import</span> defaultdict
<span class="token keyword">from</span> tf<span class="token punctuation">.</span>app <span class="token keyword">import</span> use
<span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm

A <span class="token operator">=</span> use<span class="token punctuation">(</span><span class="token string">"ETCBC/dss"</span><span class="token punctuation">,</span> hoist<span class="token operator">=</span><span class="token builtin">globals</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#here we will load the DSS (dead sea scrolls) dataa</span>

all_scrolls <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> scroll_node <span class="token keyword">in</span> tqdm<span class="token punctuation">(</span>F<span class="token punctuation">.</span>otype<span class="token punctuation">.</span>s<span class="token punctuation">(</span><span class="token string">"scroll"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#running over all scrolls available</span>
    word_line_num <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment">#first line</span>
    sub_word_num <span class="token operator">=</span> <span class="token number">1</span> <span class="token comment">#first place in the line</span>
    scroll_data <span class="token operator">=</span> process_word<span class="token punctuation">(</span>scroll_node<span class="token punctuation">,</span> word_line_num<span class="token punctuation">,</span> sub_word_num<span class="token punctuation">)</span>
    all_scrolls<span class="token punctuation">.</span>update<span class="token punctuation">(</span>scroll_data<span class="token punctuation">)</span>
</code></pre>
<p>For getting the attributes of each word we can  use the following function, which running over all of the words of a given scroll, and getting the attributes that you need.</p>
<pre class="language-python" tabindex="0"><code class="language-python">
<span class="token keyword">def</span> <span class="token function">get_scroll_and_chapter_info</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Get scroll and chapter information for a given word node."""</span>
    scroll_and_chapter <span class="token operator">=</span> A<span class="token punctuation">.</span>sectionStrFromNode<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
    scroll<span class="token punctuation">,</span> chapter_info <span class="token operator">=</span> scroll_and_chapter<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span>
    <span class="token comment"># frag_label, frag_line_num = chapter_info.split(":")</span>
    <span class="token keyword">return</span> scroll_and_chapter<span class="token punctuation">,</span> scroll
    <span class="token comment"># frag_label, frag_line_num</span>

<span class="token keyword">def</span> <span class="token function">process_word</span><span class="token punctuation">(</span>scroll_node<span class="token punctuation">,</span> word_line_num<span class="token punctuation">,</span> sub_word_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
    single_scroll_data <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> word <span class="token keyword">in</span> L<span class="token punctuation">.</span>d<span class="token punctuation">(</span>scroll_node<span class="token punctuation">,</span> otype<span class="token operator">=</span><span class="token string">"word"</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#running over all of the words of this scroll</span>
        <span class="token punctuation">(</span>
            scroll_and_chapter<span class="token punctuation">,</span>
            scroll
        <span class="token punctuation">)</span> <span class="token operator">=</span> get_scroll_and_chapter_info<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
        transcript <span class="token operator">=</span> T<span class="token punctuation">.</span>text<span class="token punctuation">(</span>word<span class="token punctuation">)</span> <span class="token comment">#get raw text</span>
        word_entry <span class="token operator">=</span> <span class="token punctuation">{</span>
            <span class="token string">"book_and_chapter"</span><span class="token punctuation">:</span> scroll_and_chapter<span class="token punctuation">,</span>
            <span class="token string">"scroll_name"</span><span class="token punctuation">:</span> scroll<span class="token punctuation">,</span>
            <span class="token string">"transcript"</span><span class="token punctuation">:</span> transcript<span class="token punctuation">,</span>

        <span class="token punctuation">}</span>
        single_scroll_data<span class="token punctuation">[</span>scroll<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>word_entry<span class="token punctuation">)</span>
    <span class="token keyword">return</span> single_scroll_data
</code></pre>
<p>But in case you want to get much rich data you can use something more complex like:</p>
<pre class="language-python" tabindex="0"><code class="language-python"><span class="token keyword">def</span> <span class="token function">get_morphological_features</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Extract morphological features for a given word node."""</span>
    <span class="token keyword">return</span> <span class="token punctuation">{</span>
        <span class="token string">"sp"</span><span class="token punctuation">:</span> F<span class="token punctuation">.</span>sp<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">"cl"</span><span class="token punctuation">:</span> F<span class="token punctuation">.</span>cl<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">"ps"</span><span class="token punctuation">:</span> F<span class="token punctuation">.</span>ps<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">"gn"</span><span class="token punctuation">:</span> F<span class="token punctuation">.</span>gn<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">"nu"</span><span class="token punctuation">:</span> F<span class="token punctuation">.</span>nu<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">"st"</span><span class="token punctuation">:</span> F<span class="token punctuation">.</span>st<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">"vs"</span><span class="token punctuation">:</span> F<span class="token punctuation">.</span>vs<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">"vt"</span><span class="token punctuation">:</span> F<span class="token punctuation">.</span>vt<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">"md"</span><span class="token punctuation">:</span> F<span class="token punctuation">.</span>md<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span>


<span class="token keyword">def</span> <span class="token function">get_scroll_and_chapter_info</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Get scroll and chapter information for a given word node."""</span>
    scroll_and_chapter <span class="token operator">=</span> A<span class="token punctuation">.</span>sectionStrFromNode<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
    scroll<span class="token punctuation">,</span> chapter_info <span class="token operator">=</span> scroll_and_chapter<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span>
    frag_label<span class="token punctuation">,</span> frag_line_num <span class="token operator">=</span> chapter_info<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">":"</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> scroll_and_chapter<span class="token punctuation">,</span> scroll<span class="token punctuation">,</span> frag_label<span class="token punctuation">,</span> frag_line_num


<span class="token keyword">def</span> <span class="token function">process_word</span><span class="token punctuation">(</span>scroll_node<span class="token punctuation">,</span> word_line_num<span class="token punctuation">,</span> sub_word_num<span class="token punctuation">)</span><span class="token punctuation">:</span>
    single_scroll_data <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> word <span class="token keyword">in</span> L<span class="token punctuation">.</span>d<span class="token punctuation">(</span>scroll_node<span class="token punctuation">,</span> otype<span class="token operator">=</span><span class="token string">"word"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token punctuation">(</span>
            scroll_and_chapter<span class="token punctuation">,</span>
            scroll<span class="token punctuation">,</span>
            frag_label<span class="token punctuation">,</span>
            frag_line_num<span class="token punctuation">,</span>
        <span class="token punctuation">)</span> <span class="token operator">=</span> get_scroll_and_chapter_info<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
        transcript <span class="token operator">=</span> T<span class="token punctuation">.</span>text<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
        lexeme <span class="token operator">=</span> F<span class="token punctuation">.</span>glex<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
        morphological_features <span class="token operator">=</span> get_morphological_features<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
        lang <span class="token operator">=</span> F<span class="token punctuation">.</span>lang<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
        srcLn <span class="token operator">=</span> F<span class="token punctuation">.</span>srcLn<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
        word_type <span class="token operator">=</span> F<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">.</span>v<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
        after <span class="token operator">=</span> F<span class="token punctuation">.</span>after<span class="token punctuation">.</span>v<span class="token punctuation">(</span>word<span class="token punctuation">)</span>

        word_entry <span class="token operator">=</span> <span class="token punctuation">{</span>
            <span class="token string">"frag_label"</span><span class="token punctuation">:</span> frag_label<span class="token punctuation">,</span>
            <span class="token string">"frag_line_num"</span><span class="token punctuation">:</span> frag_line_num<span class="token punctuation">,</span>
            <span class="token string">"word_line_num"</span><span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">(</span>word_line_num<span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">"sub_word_num"</span><span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">(</span>sub_word_num<span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">"book_and_chapter"</span><span class="token punctuation">:</span> scroll_and_chapter<span class="token punctuation">,</span>
            <span class="token string">"scroll_name"</span><span class="token punctuation">:</span> scroll<span class="token punctuation">,</span>
            <span class="token string">"transcript"</span><span class="token punctuation">:</span> transcript<span class="token punctuation">,</span>
            <span class="token string">"lex"</span><span class="token punctuation">:</span> lexeme<span class="token punctuation">,</span>
            <span class="token string">"parsed_morph"</span><span class="token punctuation">:</span> morphological_features<span class="token punctuation">,</span>
            <span class="token string">"lang"</span><span class="token punctuation">:</span> lang<span class="token punctuation">,</span>
            <span class="token string">"srcLn"</span><span class="token punctuation">:</span> srcLn<span class="token punctuation">,</span>
            <span class="token string">"type_of"</span><span class="token punctuation">:</span> word_type<span class="token punctuation">,</span>
            <span class="token string">"after"</span><span class="token punctuation">:</span> after<span class="token punctuation">,</span>
        <span class="token punctuation">}</span>
        

        <span class="token keyword">if</span> <span class="token punctuation">(</span>
            <span class="token keyword">not</span> after
        <span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># If there is no space after the word, it means it's a conjunction like ו or ב.</span>
            sub_word_num <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            sub_word_num <span class="token operator">=</span> <span class="token number">1</span>
            word_line_num <span class="token operator">+=</span> <span class="token number">1</span>

        single_scroll_data<span class="token punctuation">[</span>scroll<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>word_entry<span class="token punctuation">)</span>
    <span class="token keyword">return</span> single_scroll_data
</code></pre>
<p>That's it, from here you can do whatever you want with the data.
For more context and much ordered pipeline you can take a peek in here: <a href="https://github.com/yonatanlou/QumranNLP/blob/main/src/ETL/main_ETL.py">main_ETL.py</a>.</p>

<ul class="links-nextprev"><li class="links-nextprev-prev">← Previous<br> <a href="/eleventy-base-blog/blog/firstpost/">Moving from Jekyll to Eleventy.</a></li>
</ul>

			</heading-anchors>
		</main>

		<footer>
			<p><em>Built with <a href="https://www.11ty.dev/">Eleventy v3.0.0</a></em></p>
		</footer>

		<!-- This page `/eleventy-base-blog/blog/data-pipeline-for-qumran-scrolls/data-pipeline-for-the-qumran-scrolls-using-text-fabric/` was built on 2024-10-31T09:09:24.749Z -->
		<script type="module" src="/eleventy-base-blog/dist/rJ3_G-2ArF.js"></script>
	</body>
</html>
